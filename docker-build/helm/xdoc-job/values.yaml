# Thou! please carefully reading to avoid mistake !!!!!
# This Helm values declare all you need to deploy LV-File-Service to Developer environment, On Premise infrastructure or AWS
# Ngươi! xin vui lòng đọc kỹ để tránh sai lầm !!!!!
# Các giá trị Helm này khai báo tất cả những gì bạn cần để triển khai LV-File-Servce cho môi trường Nhà phát triển, Cơ sở hạ tầng tại chỗ hoặc AWS
#
# All pods in this deployment require below info (regardless where they would be installed):
#  Tất cả các Pod trong quá trình triển khai này đều yêu cầu thông tin bên dưới (bất kể chúng sẽ được cài đặt ở đâu):

# 1 - MongodDb database, embody by tuples of :  host, authSource, username, password. My Lambkin! do not worry about each infor that. The explaination will be show each line in this Helm Values.
#    (- Cơ sở dữ liệu MongodDb, thể hiện bằng các bộ: máy chủ, authSource, tên người dùng, mật khẩu. My Lambkin! đừng lo lắng về từng thông tin cho điều đó. Giải thích sẽ được hiển thị mỗi dòng trong Giá trị Helm này.)
# 2- Need one database in MongoDb to manage all Tenants even if File-Service just sevrve for only one Tenant. (Cần một cơ sở dữ liệu trong MongoDb để quản lý tất cả các Đối tượng thuê ngay cả khi Dịch vụ tệp chỉ phục vụ cho một Đối tượng thuê.)
# 3- At Least one ElasticSearch in which embody in tuples of: server and prefix_index ('prefix_index will be explained when it was declare')
# 4- One broker message. When Pod wanna to inform to others. The Pod could not directly  inform. It just informs to broker message and broker message will inform to another.
# 5- One temporary storage

existConfigMap: false
# if thou are deploying all Pods in K8S with an existing congfig map.
# Thou can turn this value into false
webConfig:
  isAllowDeploy: true

  # Thou! read carefully !!!!
  # For web deployment turn this value is true
  # As usual, the deployment includes web and all jobs. But sometime, web deployment and all jobs were separated.
  # For example Web in a K8S cluster A whilst All Job in K8S cluster B
  # if this value is true
  # The deployment template will include Web deployment without ingress controller config. Remember that no ingress-controller in template
  type: 'dev'
  dev:
    name: xdoc
    namespace: xdoc
    hostUrl: http://172.16.7.91/lvfile
  aws:
    name: xdoc-web
    namespace: xdoc-web
    host_url: https://apps.codx.vn/lvfile
  lacviet:
    name: xdoc-web
    namespace: xdoc-web
    hostUrl: https://codx.lacviet.vn
storageConfig:
  # This configuration  describes how and where all Pods in all deployments use shared-files when they need to process.
  type: nfs_dev
  # the type of storage
  longhorn:
    # If thou would like to use longhorn-storage turn this value into false.
      size: 30Gi
    # storage request size is at least 30 GB (why? some Pods in deployment need AI-dataset. The volume for AI-dataset, sometime reach 30GB. So, 30GB is OK )
      name: xdoc-storage
      className: longhorn-storage-delete
  nfs:
    server: 10.0.2.217
    directory: /var/nfs_share_dir_for_k8s
  nfs_dev:
    server: 172.16.7.91
    directory: /var/nfs_share_dir_for_k8s
# Make sure when thou use this value. Thou could inquire about class-name of storage by thee's DE-OP
defaults:
  replicas: 2
# if thou are deploying all Pods in K8S with 3 Worker-Nodes
# Thou could set this value is 3
# Expalination:
# The number of Pods in all deployments by default will be set by this value.
  description: "Uprate version"
  executor: "python3"
# The most Pods in all Deployment are using python3
  restartPolicy: Always
  kind: Deployment # do not care about that
  apiVersion: apps/v1 # do not care about that

# Below value is really important info. Thou must sure that value of all attributes
config:
    configName: 'dev'
    # Make sure that thou select correct configName. Wrong config properly corrupt all Pods
    lacviet:

      db:
     # At the mention at a top of file. All Pods will run if all below config are correct
     #
        admin_db_name: 'hps-file-test'
    # Tenant-manage-database even if thee's system just serve for only one Tenant
        authSource: 'hps-file-test'
    # When a Pod connects to MongoDb Database they will verify Auth at this database. As usually, authSource used to be 'admin'
        host: '10.0.2.140'
    # Server MongodDb or Mondgos router address
        password: ""
    # Password: if thee's MongoDb did not set Auth-Require, set this value is ''
        port: '27017'
    #Port: remmeber that this value should be in "'"
        username: ""
      elastic_search:
     # At the mention at a top of file. All Pods will run if all below config are correct
        prefix_index: 'lv-codx'
    # Each Tenant require a ElasticSearch index with prefix is this value
    # Exmaple Tenant name default ElasticSearch Index is 'lv-doc_default'
        server: 'http://10.0.2.140:9200'
    # Full url to ElasticSearch
      rabbitmq:
    # Broker clarification
        port: '5672'
    # Port: Thou! please quote this value
        server: 'rabbitmq.rabbitmq-dev.svc.cluster.local'
    # Server of Broker message
      temp_directory: ./brokers/tmp

    dev:

      db:
        admin_db_name: 'lv-docs'
        authSource: 'hps-file-test'
        host: '192.168.18.36'
        password: ''
        port: '27018'
        username: ''
      elastic_search:
        prefix_index: 'lv-codx'
        server: 'http://192.168.18.36:9200'
      rabbitmq:
        port: '5672'
        server: 'rabbitmq.rabbitmq-dev'
        temp_directory: './brokers/tmp'
    aws:

      db:
        authSource: hps-file-test
        host: 10.0.2.140
        password: ""
        port: '27017'
        username: ""
      elastic_search:
        prefix_index: lv-codx
        server: http://10.0.2.140:9200
      rabbitmq:
        port: '5672'
        server: rabbitmq.rabbitmq-dev.svc.cluster.local



namespace: xdoc-job-v7
# The namespace of all K8S resource



image:
  repository: nttlong/files-service-final
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.

  tag: "rc.0.5.3.0"
deployments:
  - name: files-upload
    endpoint: files_upload.py
    description: watch all upload from lv-doc-service
  - name: generate-image-from-office
    endpoint: files_generate_image_from_office.py
    description: Generate image from any office file, replicate number should be number of worker
  - name: files-generate-image-from-pdf
    endpoint: files_generate_image_from_pdf.py
  - name: files-generate-image-from-video
    endpoint: files_generate_image_from_video.py
  - name: files-ocr-pdf
    endpoint: files_ocr_pdf.py
  - name: files-generate-pdf-from-image
    endpoint: files_generate_pdf_from_image.py
  - name: files-generate-thumbs
    endpoint: files_generate_thumbs.py
  - name: files-save-custom-thumb
    endpoint: files_save_custom_thumb.py
  - name: files-save-default-thumb
    endpoint: files_save_default_thumb.py
  - name: files-save-orc-pdf-file
    endpoint: files_save_orc_pdf_file.py
  - name: files-save-search-engine
    endpoint: files_save_search_engine.py
  - name: files-extrac-text-from-image
    endpoint: files_extrac_text_from_image.py
  - name: files-clean-up
    endpoint: files_clean_up.py
    replicate: 1
  - name: files-move-tenants
    endpoint: files_move_tenants.py
  - name: files-clean-unfinished
    endpoint: files_clean_unfinish.py
    replicate: 1
  - name: fix-hanger-content
    endpoint: fix_hanger_content.py
    replicate: 1
  - name: files-layout-analizer
    endpoint: files_layout_analizer.py


jobs:
  - name: install-xdoc-torch-dataset-huggingface
    image: nttlong/xdoc-torch-dataset-huggingface:rc.0.0.4
  - name: xdoc-install-torch-dataset-layout-microsoft
    image: nttlong/xdoc-torch-dataset-layout-microsoft:rc.0.0.4
  - name: install-xdoc-torch-dataset-layouts
    image: nttlong/xdoc-torch-dataset-layouts:rc.0.0.4




#cronJobs:
#  - name: fix-elastic-search-missing-data
#    endpoint: fix_elastic_search_missing_data.py
#    description: Lỗi này là do mấy cha nội Codx đưa dữ liệu vào sai nên phải fix trước khi tìm
#    schedule: "00 00 * * *"
#    numOfPods: 1

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
#helm uninstall xdoc-job-v15
#helm install xdoc-job-v15 xdoc-job-18
#scp -r root@172.16.13.72:/home/vmadmin/python/v6/file-service-02/docker-build/helm/xdoc-job /nttlong/helm/xdoc-job